---
sidebar_position: 3
---

# Высокоуровневые концепции

`Эта документация была автоматически переведена и может содержать ошибки. Не стесняйтесь открывать Pull Request для предложения изменений.`

LlamaIndex.TS помогает вам создавать приложения, основанные на LLM (например, Q&A, чат-бот) с использованием пользовательских данных.

В этом руководстве по высокоуровневым концепциям вы узнаете:

- как LLM может отвечать на вопросы с использованием ваших собственных данных.
- ключевые концепции и модули в LlamaIndex.TS для создания собственного запроса.

## Ответы на вопросы по всем вашим данным

LlamaIndex использует двухэтапный метод при использовании LLM с вашими данными:

1. **этап индексации**: подготовка базы знаний, и
2. **этап запроса**: получение соответствующего контекста из базы знаний для помощи LLM в ответе на вопрос

![](./_static/concepts/rag.jpg)

Этот процесс также известен как Retrieval Augmented Generation (RAG).

LlamaIndex.TS предоставляет необходимый инструментарий для облегчения обоих этапов.

Давайте подробнее рассмотрим каждый этап.

### Этап индексации

LlamaIndex.TS помогает вам подготовить базу знаний с помощью набора коннекторов данных и индексов.

![](./_static/concepts/indexing.jpg)

[**Загрузчики данных**](./modules/high_level/data_loader.md):
Коннектор данных (например, `Reader`) загружает данные из различных источников данных и форматов в простое представление `Document` (текст и простая метаданные).

[**Документы / Узлы**](./modules/high_level/documents_and_nodes.md): `Document` - это общий контейнер для любого источника данных - например, PDF, вывод API или полученные данные из базы данных. `Node` - это атомарная единица данных в LlamaIndex и представляет собой "кусок" исходного `Document`. Это богатое представление, которое включает метаданные и отношения (к другим узлам), чтобы обеспечить точные и выразительные операции извлечения.

[**Индексы данных**](./modules/high_level/data_index.md):
После загрузки данных LlamaIndex помогает вам индексировать данные в формате, который легко извлекать.

Под капотом LlamaIndex разбирает исходные документы на промежуточные представления, вычисляет векторные вложения и хранит ваши данные в памяти или на диске.

"

### Этап запроса

На этапе запроса конвейер запросов извлекает наиболее релевантный контекст, учитывая запрос пользователя,
и передает его LLM (вместе с запросом) для синтеза ответа.

Это дает LLM актуальные знания, которых нет в его исходных данных обучения,
(также уменьшая галлюцинации).

Основной проблемой на этапе запроса является извлечение, оркестрация и рассуждение над (возможно, множеством) баз знаний.

LlamaIndex предоставляет составные модули, которые помогают вам создавать и интегрировать конвейеры RAG для Q&A (движок запросов), чат-бота (чат-движок) или в качестве части агента.

Эти строительные блоки могут быть настроены для отражения предпочтений ранжирования, а также составлены для рассуждения над несколькими базами знаний структурированным образом.

![](./_static/concepts/querying.jpg)

#### Строительные блоки

[**Извлекатели**](./modules/low_level/retriever.md):
Извлекатель определяет, как эффективно извлекать соответствующий контекст из базы знаний (т.е. индекса) при заданном запросе.
Конкретная логика извлечения отличается для разных индексов, наиболее популярным является плотное извлечение из векторного индекса.

[**Синтезаторы ответов**](./modules/low_level/response_synthesizer.md):
Синтезатор ответов генерирует ответ от LLM с использованием запроса пользователя и заданного набора извлеченных фрагментов текста.

"

#### Конвейеры

[**Движки запросов**](./modules/high_level/query_engine.md):
Движок запросов - это конвейер от начала до конца, который позволяет вам задавать вопросы о ваших данных.
Он принимает естественноязыковой запрос и возвращает ответ, вместе с извлеченным контекстом, переданным LLM.

[**Чат-движки**](./modules/high_level/chat_engine.md):
Чат-движок - это конвейер от начала до конца для ведения разговора с вашими данными
(множество вопросов и ответов вместо одного вопроса и ответа).

"
