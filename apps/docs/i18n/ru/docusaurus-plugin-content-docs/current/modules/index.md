# Основные модули

`Эта документация была автоматически переведена и может содержать ошибки. Не стесняйтесь открывать Pull Request для предложения изменений.`

LlamaIndex.TS предлагает несколько основных модулей, разделенных на модули высокого уровня для быстрого начала работы и модули низкого уровня для настройки ключевых компонентов по мере необходимости.

## Модули высокого уровня

- [**Документ**](./high_level/documents_and_nodes.md): Документ представляет собой текстовый файл, файл PDF или другой непрерывный кусок данных.

- [**Узел**](./high_level/documents_and_nodes.md): Основной строительный блок данных. Обычно это части документа, разделенные на управляемые куски, достаточно маленькие, чтобы их можно было подать на модель встраивания и LLM.

- [**Reader/Loader**](./high_level/data_loader.md): Reader или Loader - это то, что принимает документ в реальном мире и преобразует его в класс Document, который затем может быть использован в вашем индексе и запросах. В настоящее время мы поддерживаем обычные текстовые файлы и PDF-файлы, а также много других форматов.

- [**Индексы**](./high_level/data_index.md): индексы хранят узлы и вложения этих узлов.

- [**QueryEngine**](./high_level/query_engine.md): Query Engine - это то, что генерирует запрос, который вы вводите, и возвращает результат. Query Engine обычно объединяет заранее созданный запрос с выбранными узлами из вашего индекса, чтобы предоставить LLM контекст, необходимый для ответа на ваш запрос.

- [**ChatEngine**](./high_level/chat_engine.md): ChatEngine помогает вам создать чат-бота, который будет взаимодействовать с вашими индексами.

## Модули низкого уровня

- [**LLM**](./low_level/llm.md): Класс LLM - это унифицированный интерфейс для больших поставщиков языковых моделей, таких как OpenAI GPT-4, Anthropic Claude или Meta LLaMA. Вы можете создать подкласс для написания коннектора к своей собственной большой языковой модели.

- [**Embedding**](./low_level/embedding.md): Встраивание представляется в виде вектора чисел с плавающей запятой. Наша модель встраивания по умолчанию - это OpenAI's text-embedding-ada-002, и каждое встраивание, которое она генерирует, состоит из 1536 чисел с плавающей запятой. Еще одна популярная модель встраивания - это BERT, который использует 768 чисел с плавающей запятой для представления каждого узла. Мы предоставляем несколько утилит для работы с встраиваниями, включая 3 варианта расчета сходства и максимальную маржинальную релевантность.

- [**TextSplitter/NodeParser**](./low_level/node_parser.md): Стратегии разделения текста крайне важны для общей эффективности поиска встраивания. В настоящее время у нас есть значение по умолчанию, но нет универсального решения. В зависимости от исходных документов вы можете использовать разные размеры и стратегии разделения. В настоящее время мы поддерживаем разделение по фиксированному размеру, разделение по фиксированному размеру с перекрывающимися секциями, разделение по предложению и разделение по абзацу. Разделитель текста используется NodeParser при разделении `Document` на `Node`.

- [**Retriever**](./low_level/retriever.md): Retriever - это то, что фактически выбирает узлы для извлечения из индекса. Здесь вы можете попробовать извлечь больше или меньше узлов для каждого запроса, изменить функцию сходства или создать собственный извлекатель для каждого отдельного случая использования в вашем приложении. Например, вы можете создать отдельный извлекатель для кодового содержимого и текстового содержимого.

- [**ResponseSynthesizer**](./low_level/response_synthesizer.md): ResponseSynthesizer отвечает за преобразование строки запроса и использование списка `Node` для генерации ответа. Это может быть в виде итерации по всем контекстам и уточнения ответа или построения дерева сводок и возврата корневой сводки.

- [**Storage**](./low_level/storage.md): В какой-то момент вам захочется сохранить свои индексы, данные и векторы, а не запускать модели встраивания каждый раз. IndexStore, DocStore, VectorStore и KVStore - это абстракции, позволяющие вам это сделать. Вместе они формируют контекст хранения. В настоящее время мы позволяем сохранять встраивания в файлах на файловой системе (или виртуальной файловой системе в памяти), но мы также активно добавляем интеграции с векторными базами данных.
